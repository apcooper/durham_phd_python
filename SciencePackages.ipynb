{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Packages for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this notebook is a bit different to the others. \n",
    "\n",
    "At this point you should be confident installing new python packages for yourself (using `pip`, `conda` and/or installing directly from source) and finding documentation for these online.\n",
    "\n",
    "The point of this notebook is to introduce some potentially useful packages. Some of these are installed on the JupyterHub server, and some aren't.\n",
    "\n",
    "On our local JupyterHub server, you can install local packages using `pip3` with the `--user` option. This puts the packages into `~/.local/site/lib/site-packages`. You don't have access to a terminal, but you can issue these commands from within the notebook by starting them with a `!`, which is the Jupyter notebook magic for running shell commands.\n",
    "\n",
    "For example, to install the [`corner`](http://corner.readthedocs.io/en/latest/) package (described below), create and run a new cell with:\n",
    "\n",
    "`!pip3 install --user corner`\n",
    "\n",
    "You should then be able to `import corner`. If you still get an import error (and have double checked the name of the module to import is correct), try restarting the kernel.\n",
    "\n",
    "By this stage, though, you should be familiar with how to install and work with packages on your own machine.\n",
    "\n",
    "This notebook only has minimal examples for each package -- most have extensive tutorials online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS FIRST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy` package has more complicated numerical procedures that work (mostly) with `numpy` arrays. I've only ever used a small fraction of them! The following are examples of some `scipy` routines I find useful. `scipy` is already installed on our JupyterHub -- ususally you can expect `scipy` wherever you find `numpy`.\n",
    "\n",
    "Section 1.5 of http://www.scipy-lectures.org/ introduces the many different sub-packages that are part of SciPy. Those notes also give a very comprehensive introduction to Python, Matplotlib and Numpy at a more complete and technical level than our notebooks.\n",
    "\n",
    "The following sections introduce some of the more frequently encoutered functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you're familiar with the basic idea of interpolation. The `scipy.interpolate.interp1d` function returns another function that takes a set of points `x` and associated 'measured values' `y` and returns an interpolated value of `y` at any intermediate value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.interpolate as spi\n",
    "\n",
    "# 10 sparsely-sampled 'data' points\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0,2*np.pi,10)\n",
    "pl.scatter(x,np.sin(x),c='k',label='data')\n",
    "\n",
    "# Linearly interpolate between these points.\n",
    "linear_interp = spi.interp1d(x,np.sin(x),kind='linear')\n",
    "cubic_interp  = spi.interp1d(x,np.sin(x),kind='cubic')\n",
    "\n",
    "xx  = np.linspace(x.min(),x.max(),100)\n",
    "_xx = np.linspace(-np.pi/2.0,2.5*np.pi) # Show true curve over a wider range in x\n",
    "pl.plot(_xx,np.sin(_xx),c='k',ls='--',label='(true)')\n",
    "pl.plot(xx,linear_interp(xx),c='r',label='linear interpolation')\n",
    "pl.plot(xx,cubic_interp(xx),c='b',label='cubic interpolation')\n",
    "\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('sin(x)')\n",
    "\n",
    "pl.legend(loc='upper right',frameon=False);\n",
    "pl.xlim(0,2*np.pi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Have a look at the documentation for `interp1d` to figure out why the following cell raises `ValueError` and what your options ar for fixing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10 sparsely-sampled 'data' points\n",
    "np.random.seed(42)\n",
    "x             = np.random.uniform(0,2*np.pi,10)\n",
    "linear_interp = spi.interp1d(x,np.sin(x),kind='linear')\n",
    "\n",
    "x_to_interpolate_at = np.arange(0,3*np.pi,np.pi/2.0)\n",
    "print(linear_interp(x_to_interpolate_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common use of interpolation is to invert functions numerically, i.e. to construct a continuous function that returns `x(y)` for any `y` using data sampling `y(x)` at a few values of `x`. The example in the next cell inverts `f(x) = sin(x)` (which we know is `arcsin(x)`). In the more general case we might not know (or even have) the analytic form of the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.interpolate as spi\n",
    "\n",
    "theta_min  = -np.pi/2.0\n",
    "theta_max  =  np.pi/2.0\n",
    "theta_step =  np.pi/32.0\n",
    "\n",
    "# Our simple numerical inverse of sine function\n",
    "y = np.arange(theta_min,theta_max+theta_step,theta_step)\n",
    "linear_interp = spi.interp1d(np.sin(y),y,kind='linear',bounds_error=False)\n",
    "\n",
    "# Plot the inverse\n",
    "x = np.arange(-1,1+0.1,0.1)\n",
    "pl.plot(x,linear_interp(x),c='r',label='interpolated inverse')\n",
    "\n",
    "# Compare with the actual arcsin function\n",
    "pl.scatter(x,np.arcsin(x),c='k',label='true inverse')\n",
    "\n",
    "pl.xlabel('sin(x)')\n",
    "pl.ylabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple example numerically integrates `y=x**2` between 4 and 6, just to demonstrate how to do this with `scipy`. Practical uses of numerical integration often turn out to be a lot more complicated!\n",
    "\n",
    "The main point of this example is that numerical integration by Gaussian quadrature often works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.integrate as spint\n",
    "\n",
    "x = np.linspace(0,10,100)\n",
    "\n",
    "pl.figure(figsize=(6,6))\n",
    "pl.plot(x,x**2);\n",
    "pl.fill_between(x,x**2,0,where=((x>=4) & (x<=6)),alpha=0.4,color='c')\n",
    "\n",
    "# The analytic solution\n",
    "analytic = lambda x: (1/3.0)*x**3\n",
    "print('Analytic solution: {}'.format(analytic(6)-analytic(4)))\n",
    "\n",
    "# The numerical solution using adaptive gaussian quadrature\n",
    "func = lambda t: t**2 # the function to integrate\n",
    "integral_quad, quad_err = spint.quad(func,4,6)\n",
    "print('Quad. solution:    {}'.format(integral_quad))\n",
    "\n",
    "# The numerical solution using samples of the function\n",
    "mask = (x>=4) & (x<=6)\n",
    "print('Number of samples in interval = %d'%(np.sum(mask)))\n",
    "integral_simps = spint.simps(x[mask]**2,x[mask])\n",
    "print('Simpson solution:  {}'.format(integral_simps))\n",
    "\n",
    "pl.ylim(0,100)\n",
    "pl.xlabel('x')\n",
    "pl.ylabel('x**2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware of that functions exist to do Romberg integration: `scipy.integrate.romberg` (continuous functions) and `scipy.integrate.romb` (same thing for discrete samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbours with KD Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KDTree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html) (a class defined in `scipy.spatial.kdtree`) provides an efficient way to search for neighbours of points in a multi-dimensional coordinate space. This example shows how to do this for a 2D array of coordinates (incidentally, it also shows how to draw a circle on a plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.kdtree import KDTree\n",
    "\n",
    "np.random.seed(42)\n",
    "xy = np.random.normal(1,1,(100,2))\n",
    "kd = KDTree(xy) # construct the tree by passing an array of coordinates\n",
    "\n",
    "# Show the points on a scater plot\n",
    "pl.figure(figsize=(6,6))\n",
    "pl.scatter(xy[:,0],xy[:,1],s=20,edgecolor='None',c='k')\n",
    "\n",
    "# Find all the points within radius r=1 of the first point\n",
    "radius = 1\n",
    "idx  = kd.query_ball_point(xy[0,:],radius) # returns the index of each point in xy within r<=1 of element 0\n",
    "\n",
    "# Highlight the neighbours on the plot\n",
    "pl.scatter(xy[0,0],xy[0,1],c='r',s=20,zorder=10,edgecolor='None')\n",
    "for i in idx[:]:\n",
    "    if i != 0:\n",
    "        pl.scatter(xy[i,0],xy[i,1],c='lime',s=60,edgecolor='None',zorder=0)\n",
    "\n",
    "# Draw a circle of radius 1\n",
    "ax = pl.gca()\n",
    "ax.add_artist(pl.Circle([xy[0,0],xy[0,1]],radius=radius,color='purple',alpha=0.1,zorder=-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other functions and techniques associated with the KDTree object.\n",
    "\n",
    "In the same package there is also `cKDTree`, which is an optimized version of `KDTree` for very large datasets, with fewer functions than `KDTree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important topics in `scipy` include:\n",
    "\n",
    "* Linear algebra\n",
    "* Signal processing (resampling, filtering)\n",
    "* Fourier transformations\n",
    "* Optimization (finding minima/maxima of functions) and fitting\n",
    "\n",
    "To learn more about these, follow the tutorial here:\n",
    "http://www.scipy-lectures.org/intro/scipy.html. \n",
    "\n",
    "There are also some exercises to work through:\n",
    "http://www.scipy-lectures.org/intro/scipy.html#summary-exercises-on-scientific-computing\n",
    "\n",
    "If you don't have access to a terminal, you can copy the examples and exercises to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKits are packages that integrate directly with SciPy to provide (in theory) well-tested and robust code for many common scientific tasks. See this link for a description: https://scikits.appspot.com/about. Each has to be installed as a separate package. There are a huge number of these (complete list here: https://scikits.appspot.com/scikits) with varying degrees of comprehensiveness/quality.\n",
    "\n",
    "Not every major scientific package is a SciKit. For example, Astropy, the main package of general purpose astronomy modules, is not. Being a SciKit does not really bestow any particular special status on a package.\n",
    "\n",
    "From experience, the most relevant SciKits for astronomers are `scikit-learn` (by far the most common) and `scikit-image` (less often encountered but potentially useful). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/\n",
    "\n",
    "This package contains functions for data mining and machine learning. \n",
    "\n",
    "It is the basis for a number of other packages, including the astronomy-specific machine learning package `astroML` http://www.astroml.org/.\n",
    "\n",
    "Install (from this notebook): `!pip3 install --user scikit-learn`\n",
    "Import: `import sklearn`\n",
    "\n",
    "There is a bewildering array of different tools in `scikit-learn`. Using any of them requires some understanding of the underlying statistical concepts.\n",
    "\n",
    "Tutorials: http://scikit-learn.org/stable/tutorial/index.html\n",
    "\n",
    "The following cell gives a very simple example of using the `GaussianMixture` class from `sklean.mixture` to fit a mixture-of-Gaussians model to some data. In this case the data is sampled from a combination of 2 Gaussian components, so we expect a 2-component mixture model to be a good fit. \n",
    "\n",
    "This example deliberately uses more explicit steps than some of the examples of this function you can find online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn import mixture\n",
    "\n",
    "# Define a true distribution with two components\n",
    "true_means  = np.array([-5.0, 4.0])\n",
    "true_sigmas = np.array([ 6.0, 3.0])\n",
    "\n",
    "# True weights of each component\n",
    "true_weights = np.array([0.25,0.75])\n",
    "\n",
    "print('True distribution:')\n",
    "print('%-15s'%('Means:'),   true_means)\n",
    "print('%-15s'%('Sigmas:'),  true_sigmas)\n",
    "print('%-15s'%('Weights:'), true_weights)\n",
    "\n",
    "# Ratio of component 2 to component 1:\n",
    "true_ratio = true_weights[1]/true_weights[0]\n",
    "\n",
    "# Number of samples to draw for each component\n",
    "N1    = 200\n",
    "N2    = int(true_ratio*N1)\n",
    "\n",
    "# Sample from each of the components of the true distribution\n",
    "np.random.seed(42)\n",
    "s1 = np.random.normal(true_means[0],true_sigmas[0],size=N1)\n",
    "s2 = np.random.normal(true_means[1],true_sigmas[1],size=N2)\n",
    "\n",
    "# Combine the two samples -- requires some care because\n",
    "# GaussianMixture requires an input array of shape\n",
    "# (nsamples,nfeatures)\n",
    "\n",
    "# Here, nfeatures can be throught of as ndimenions.\n",
    "# Since we're fitting a 1d ditribution, nfeatures is 1\n",
    "# So the shape has to be (N1+N2, 1), not (N1+N2,)\n",
    "S  = np.atleast_2d(np.concatenate([s1,s2])).T\n",
    "\n",
    "# Fit a 2-component model to the sampled data\n",
    "# There are many more possible options to GaussianMixture\n",
    "mixture_model = mixture.GaussianMixture(n_components=2)\n",
    "mixture_model.fit(S)\n",
    "\n",
    "print()\n",
    "print('Fit converged?', mixture_model.converged_)\n",
    "print()\n",
    "\n",
    "# Plots\n",
    "pl.figure(figsize=(8,6))\n",
    "\n",
    "# Normalized histogram of the sampled data\n",
    "bins  = np.arange(-20,20,0.5)\n",
    "h1, _ = np.histogram(s1, bins=bins,normed=True)\n",
    "h2, _ = np.histogram(s2, bins=bins,normed=True)\n",
    "H,  _ = np.histogram(S,  bins=bins,normed=True)\n",
    "pl.plot(bins[:-1], H, drawstyle='steps-post',lw=3,c='green')\n",
    "\n",
    "# Plot the underlying distributions as solid lines\n",
    "x  = np.linspace(-20,20,100)\n",
    "y1 = true_weights[0]*norm.pdf(x, true_means[0], true_sigmas[0])\n",
    "y2 = true_weights[1]*norm.pdf(x, true_means[1], true_sigmas[1])\n",
    "Y  = y1 + y2\n",
    "\n",
    "pl.plot(x, y1 ,c='k',alpha=0.5,label='True 1')\n",
    "pl.plot(x, y2 ,c='k',alpha=0.5,label='True 2')\n",
    "pl.plot(x, Y,  c='r',lw=2,label='True')\n",
    "pl.legend(fontsize=12);\n",
    "\n",
    "# Extract the model components\n",
    "# The flattening and indexing here is just because means_ and covariances_ \n",
    "# have shapes that reflect their generalization to N components.\n",
    "model_means   = mixture_model.means_.flatten()\n",
    "model_sigmas  = np.sqrt(mixture_model.covariances_[:,0]).flatten()\n",
    "model_weights = mixture_model.weights_\n",
    "\n",
    "print('Model distributuion:')\n",
    "print('%-15s'%('Means:'),   model_means)\n",
    "print('%-15s'%('Sigmas:'),  model_sigmas)\n",
    "print('%-15s'%('Weights:'), model_weights)\n",
    "\n",
    "# Model components (note scale = sqrt(variance))\n",
    "M1 = model_weights[0]*norm.pdf(x, model_means[0], model_sigmas[0])\n",
    "M2 = model_weights[1]*norm.pdf(x, model_means[1], model_sigmas[1])\n",
    "M  = M1 + M2\n",
    "\n",
    "# Plot the model\n",
    "pl.plot(x, M1,c='b',alpha=0.5,label='Model 1',ls='--')\n",
    "pl.plot(x, M2,c='b',alpha=0.5,label='Model 2',ls='--')\n",
    "pl.plot(x, M, c='b',label='Model',lw=2)\n",
    "\n",
    "pl.xlabel('x',fontsize=12)\n",
    "pl.ylabel('P(x)',fontsize=12)\n",
    "pl.legend(fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the evidence for different numbers of components in fitting the data by using the 'AIC' measure. The best model minimizes the AIC.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Akaike_information_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aics = list()\n",
    "for ncomp in range(1,10):\n",
    "    model = mixture.GaussianMixture(n_components=ncomp)\n",
    "    model.fit(S)\n",
    "    aics.append(model.aic(S))\n",
    "    \n",
    "pl.plot(range(1,10),aics)\n",
    "pl.ylabel('AIC')\n",
    "pl.xlabel('N components');\n",
    "\n",
    "print('The best Gaussian mixture model has %d components.'%(np.argmin(aics)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is probably the best format for large binary datasets with complicated structure (better than fits, because FITS gets rather tricky for multidimensional array data, and for data split into many small groups). \n",
    "\n",
    "Many simulation codes output their results as HDF5. If you're saving more than `>100 Mb` of data, HDF5 is more practical than `pickle` or `numpy.savez`, and more portable.\n",
    "\n",
    "HDF5 files are collections of arrays (called 'datasets') arranged in a hierarchical structure a bit like a file system (rather than directories with files in them, there are 'groups' with datasets in them). You could also think of them as being similar to Python `dicts`. It's possible to associate 'attributes' ('headers', like comments or units) with individual datasets and to compress them to save disk space.\n",
    "\n",
    "Strangely, there are *two* almost equally good packages to deal with HDF5 in python: [`h5py`](http://www.h5py.org/) and [`PyTables`](http://www.pytables.org/). The style of these packages is slightly different, with `PyTables` being slightly closer to the way of working with hdf5 in `C` and focusing on high performance, and `h5py` being more abstract and closer to other python packages like `numpy`, focused on ease of use. The differences are described by the authors of `PyTables` [here](http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project) and by the `h5py` team [here](http://docs.h5py.org/en/latest/faq.html#what-s-the-difference-between-h5py-and-pytables). For general use either is probably fine, because `PyTables` is pretty easy to use now too.\n",
    "\n",
    "For this example you need to install `h5py` and follow the [quick start page](http://docs.h5py.org/en/latest/quick.html) and [instructions for readoing and writing datasets](http://docs.h5py.org/en/latest/high/dataset.html) to understand how to create an HDF5 file and read it back in.\n",
    "\n",
    "Note that `PyTables` is provided by the package `tables` (without the `py`!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Fake data\n",
    "mass_dm    = np.random.random(100)\n",
    "mass_stars = np.random.random(100)\n",
    "age_stars  = np.random.random(100)\n",
    "\n",
    "# Write a file\n",
    "f = h5py.File('mydata.hdf5', mode='w')\n",
    "try: # (see following notes for an explanation of try..finally..)\n",
    "    f.create_dataset('/dm/mass',    data=mass_dm) # we give the 'path' to the data in each case\n",
    "    f.create_dataset('/stars/mass', data=mass_stars)\n",
    "    f.create_dataset('/stars/age',  data=age_stars)\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Read the same file back\n",
    "g = h5py.File('mydata.hdf5',mode='r')\n",
    "try:\n",
    "    stellar_mass_dataset = g['/stars/mass'] # This deliberately looks like accessing a dict object\n",
    "    \n",
    "    # As long as the file is open, the array can be accessed.\n",
    "    print('type of stellar mass dataset: {}'.format(type(stellar_mass_dataset)))\n",
    "    print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))\n",
    "    print('First 10 elements of stellar mass: {}'.format(stellar_mass_dataset[0:10]))\n",
    "    \n",
    "    # If you want to use the data after the file is closed,\n",
    "    # store its `value`\n",
    "    stellar_mass_values = stellar_mass_dataset.value[0:10]\n",
    "finally:\n",
    "    g.close();\n",
    "print(\"After we've closed the file...\")\n",
    "print('stellar_mass_values is {}'.format(stellar_mass_values))\n",
    "print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the `try..finally` block in that example? Remember that the point of the `try` block is to capture exceptions that happen inside it so you can do something with them rather than stopping the code. The exceptions can be handled with an `except` clause.\n",
    "\n",
    "***The code in the `finally` part of the block is *guaranteed* to be executed regardless of what happens in the `try..except..` part.***\n",
    "\n",
    "In the example above, we don't handle any exceptions with `except`, but we do use the `try` to make sure that the file is closed properly (with `f.close()`) even if something goes wrong.\n",
    "\n",
    "You don't *have* to do this, but in the past I've found it's possible to corrupt hdf5 files that are being written to if something goes wrong during the write and the file isn't properly closed. \n",
    "\n",
    "It's common to modify HDF5 files that already exist. This is one of the advantages of the 'directory-like' structure of HDF5: if you compute something based on the data in a file (for example, if you compute the magnitudes of a bunch of stars for which you already have fluxes) then it's very easy to store that information in the same file as a separate dataset (it's a real pain to do that with FITS). In such casess you want to be careful not to corrupt the file, so I always wrap the access to the file in a `try..finally` to be on the safe side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also try the same thing with `pytables` (tutorial [here](http://www.pytables.org/usersguide/tutorials.html)). This simple example looks very similar between the two packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb # Note, not pytables\n",
    "\n",
    "# Fake data\n",
    "mass_dm    = np.random.random(100)\n",
    "mass_stars = np.random.random(100)\n",
    "age_stars  = np.random.random(100)\n",
    "\n",
    "f = tb.open_file('my_data.hdf5',mode='w')\n",
    "try:\n",
    "    # createparents=True makes sure the 'path' to the dataset exists\n",
    "    f.create_array('/dm',   'mass',obj=mass_dm,createparents=True)\n",
    "    f.create_array('/stars','mass',obj=mass_stars,createparents=True)\n",
    "    f.create_array('/stars','age', obj=age_stars,createparents=True)\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Read the same file back\n",
    "g = tb.open_file('mydata.hdf5',mode='r')\n",
    "try:\n",
    "    # In pytables you can either use a object-like syntax for getting groups/datasets...\n",
    "    stellar_mass_dataset = g.root.stars.mass\n",
    "    # ... or this way, which looks more like pytables:\n",
    "    stellar_mass_dataset = g.get_node('/stars/mass')\n",
    "    \n",
    "    # As long as the file is open, the array can be accessed.\n",
    "    print('type of y: {}'.format(type(stellar_mass_dataset)))\n",
    "    print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))\n",
    "    print('First 10 elements of stellar_mass_dataset: {}'.format(stellar_mass_dataset[0:10]))\n",
    "    \n",
    "    # In pytables, slicing the array gives the data directly\n",
    "    # To get the whole array, do values = dataset[:] or values = dataset.read()\n",
    "    stellar_mass_values = stellar_mass_dataset[0:10]\n",
    "finally:\n",
    "    g.close();\n",
    "print(\"After we've closed the file...\")\n",
    "print('stellar_mass_values is {}'.format(stellar_mass_values))\n",
    "print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These might not be installed on your system, if not then the following\n",
    "# shell commands will not work.\n",
    "\n",
    "!echo 'Output of h5ls:'\n",
    "!h5ls 'mydata.hdf5'\n",
    "!echo\n",
    "!echo 'Output of h5ls/stars:'\n",
    "!h5ls 'mydata.hdf5/stars'\n",
    "!echo\n",
    "!echo 'Output of h5dump -g /dm (to show the contents of the /dm group only)'\n",
    "!h5dump -g /dm 'mydata.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading binary files written by Fortran codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever need to read arrays directly from binary files (this is how lots of simulation data used to be stored, before the likes of HDF5), then `numpy.fromfile` is the tool to use. `numpy` arrays have a method `tofile` that can be used in a similar way to write your own raw binary files if you ever need to, but in most cases you shouldn't need to: always use `numpy.save`, `hdf5` or some other portable, easy-to-use format for working with binary data. Unfortunately other people will not follow this advice, so you may still need to know how to read binary files.\n",
    "\n",
    "The `Fortran` language has a special `formatted` binary output, where each block of data is preceded by two bytes holding the length of the block in bytes, which I call the 'header'. An identical 'footer' is written at the end of the data block. The point of these is to allow you to move quickly through a binary file, because you only have to read the header blocks to 'skip' over the data blocks. The point of the footer is a consistency check that you've read the right number of data bytes. The following example writes such a file and then reads it again, to illustrate how `tofile` and `fromfile` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some data\n",
    "x = np.arange(0,100,dtype=np.float64)\n",
    "\n",
    "# Write data to a file\n",
    "with open('my_binary_file.dat','wb') as f:\n",
    "    np.array(len(x),dtype='i8').tofile(f) # Header byte \n",
    "    x.tofile(f)                           # The array\n",
    "    np.array(len(x),dtype='i8').tofile(f) # Footer byte\n",
    "\n",
    "# Read the file\n",
    "with open('my_binary_file.dat','rb') as f: # Open in binary read mode\n",
    "    block_length_start = np.fromfile(f,'i8',1)\n",
    "    data               = np.fromfile(f,'f8',int(block_length_start))\n",
    "    block_length_end   = np.fromfile(f,'i8',1)\n",
    "    assert(block_length_start == block_length_end) # Assert thrown an exception if its condition is false\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` is a general-purpose package for statistics and databases organized as tables. It's not all that widely used in astronomy, but it's very widely used for 'data science' in general.\n",
    "\n",
    "[Here](http://pandas.pydata.org/pandas-docs/stable/overview.html) is an overview of what can be done with `pandas`. Some of the functions overlap with more astronomy-specific versions in `astropy` (for example, reading and manipulating data tables). I don't find `pandas` very easy to understand or use.\n",
    "\n",
    "There is a collection of `pandas` tutorials here: http://pandas.pydata.org/pandas-docs/stable/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC and emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's becoming increasingly common to work with Markov Chain Monte Carlo sampling, particularly in making maximum-likelihood estimates of parameters in theoretical models. There are at least two major Python packages for this, with  different emphasis on the approach and hence on the problems they're most suited to:\n",
    "\n",
    "PyMC: https://pymc-devs.github.io/pymc/\n",
    "emcee: http://dfm.io/emcee/current/\n",
    "\n",
    "They both have excellent tutorials to get started.\n",
    "https://github.com/pymc-devs/pymc/wiki\n",
    "http://dfm.io/emcee/current/user/quickstart/\n",
    "\n",
    "`PyMC` tries to be flexible and include lots of tools for specific problems, `emcee` is more of a brute-force 'sledgehammer'.\n",
    "\n",
    "If you make lots of multidimensional likelihood plots, you may be interested in the `corner` package: http://corner.readthedocs.io/en/latest/\n",
    "\n",
    "You may also want to investigate MultiNest, for which there is a Python wrapper:\n",
    "https://johannesbuchner.github.io/PyMultiNest/pymultinest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
