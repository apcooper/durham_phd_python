{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Session 2](#1.-Session-2)\n",
    "* [2. My solution to the challenge from Session 1](#2.-My-solution-to-the-challenge-from-Session-1)\n",
    "\t* [2.1 Command line arguments](#2.1-Command-line-arguments)\n",
    "* [3. Other useful standard library modules](#3.-Other-useful-standard-library-modules)\n",
    "\t* [3.1 `time` and `datetime`](#3.1-time-and-datetime)\n",
    "\t* [3.2 `argparse`](#3.2-argparse)\n",
    "\t* [3.3 `glob`](#3.3-glob)\n",
    "\t* [3.4 `subprocess`](#3.4-subprocess)\n",
    "* [4. More matplotlib](#4.-More-matplotlib)\n",
    "\t* [4.1 Histograms](#4.1-Histograms)\n",
    "\t* [4.2 Legends](#4.2-Legends)\n",
    "\t* [4.3 Grids of plots](#4.3-Grids-of-plots)\n",
    "\t* [4.4 Fill Between](#4.4-Fill-Between)\n",
    "* [5. More `numpy`](#5.-More-numpy)\n",
    "\t* [5.1 Copies vs. references](#5.1-Copies-vs.-references)\n",
    "\t* [5.2 Sorting arrays](#5.2-Sorting-arrays)\n",
    "\t* [5.3 Concatenating, stacking and splitting](#5.3-Concatenating,-stacking-and-splitting)\n",
    "\t* [5.4 Creating arrays, repeat, reshape](#5.4-Creating-arrays,-repeat,-reshape)\n",
    "\t* [5.5 Record arrays (recarrays)](#5.5-Record-arrays-%28recarrays%29)\n",
    "\t* [5.6 Logical masks and crazy floating point values (`nan` and `inf`)](#5.6-Logical-masks-and-crazy-floating-point-values-%28nan-and-inf%29)\n",
    "\t* [5.7 `numpy.where` with 2d arrays](#5.7-numpy.where-with-2d-arrays)\n",
    "\t* [5.8 Unique values](#5.8-Unique-values)\n",
    "\t* [5.9 Testing if values are in another array using in1d](#5.9-Testing-if-values-are-in-another-array-using-in1d)\n",
    "\t* [5.10 Finding locations of specific values in arrays](#5.10-Finding-locations-of-specific-values-in-arrays)\n",
    "* [6. Scipy](#6.-Scipy)\n",
    "\t* [6.1 Finding neighbours with KD Trees](#6.1-Finding-neighbours-with-KD-Trees)\n",
    "\t* [6.2 Interpolating](#6.2-Interpolating)\n",
    "\t* [6.3 Integration](#6.3-Integration)\n",
    "* [7. Astropy](#7.-Astropy)\n",
    "\t* [7.1 Tables](#7.1-Tables)\n",
    "\t\t* [7.1.1 Sky coordinates](#7.1.1-Sky-coordinates)\n",
    "* [8. Challenges](#8.-Challenges)\n",
    "\t* [8.1 Write a 'particle data' package](#8.1-Write-a-'particle-data'-package)\n",
    "\t* [8.2 Fitting](#8.2-Fitting)\n",
    "\t* [8.3 Fancy 2D Histogram](#8.3-Fancy-2D-Histogram)\n",
    "\t* [8.4 Matching values in huge arrays of integers](#8.4-Matching-values-in-huge-arrays-of-integers)\n",
    "* [9. Next week](#9.-Next-week)\n",
    "* [10. Appendix](#10.-Appendix)\n",
    "\t* [10.1 Making your own Python packages](#10.1-Making-your-own-Python-packages)\n",
    "\t* [10.2 Implicit lists of arguments and sets of keyword arguments](#10.2-Implicit-lists-of-arguments-and-sets-of-keyword-arguments)\n",
    "\t* [10.3 Broadcasting](#10.3-Broadcasting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's goals:\n",
    " - Look at the solution to the challenge from last time\n",
    " - Have a quick look at four more useful packages from the standard library that:\n",
    "     - time how long code takes to run\n",
    "     - read command line arguments\n",
    "     - search for files and folders matching a pattern\n",
    "     - call shell commands from inside python scripts\n",
    " - See how to do more with matplotlib, including:\n",
    "     - make histogram plots and plots with multiple panels\n",
    "     - make cosmetic changes to plot legends and shade between lines\n",
    " - Learn about some more common operations on numpy arrays.\n",
    " - Learn about sorting arrays, and searching in sorted arrays: useful for matching numbers in catalogues.\n",
    " - Introduce functions from `scipy` to:\n",
    "     - find distances between large numbers of points efficiently\n",
    "     - interpolate functions\n",
    "     - do numerical integration\n",
    " - Introduce the `astropy` package: we'll look at data tables, reading tables from FITS files, and dealing with spherical coordinates on the sky.\n",
    "     \n",
    "In this session, especially the challenges, try experiment with **writing programs in a text editor** and running them either directly with `python` or through an IPython session using `%run`. This is the way most people usually interact with Python. The Jupyter notebook is only to give examples.\n",
    "\n",
    "------\n",
    "\n",
    "The next cell imports all the modules used in the notebook, and creates some data that is used in some of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##############\n",
    "# Notebook setup -- just execute this cell and move on!\n",
    "# ##############\n",
    "\n",
    "# Import everything to start with\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "pl.rc('font', family='serif')\n",
    "pl.rc('font', size='10')\n",
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set up some data for later\n",
    "# (data from Hogg, Bovy & Lang 2010)\n",
    "line_data = np.array([(1, 201.0, 592.0, 9.0, 61.0, -0.84),\n",
    "       (2, 244.0, 401.0, 4.0, 25.0, 0.31),\n",
    "       (3, 47.0, 583.0, 11.0, 38.0, 0.64),\n",
    "       (4, 287.0, 402.0, 7.0, 15.0, -0.27),\n",
    "       (5, 203.0, 495.0, 5.0, 21.0, -0.33),\n",
    "       (6, 58.0, 173.0, 9.0, 15.0, 0.67),\n",
    "       (7, 210.0, 479.0, 4.0, 27.0, -0.02),\n",
    "       (8, 202.0, 504.0, 4.0, 14.0, -0.05),\n",
    "       (9, 198.0, 510.0, 11.0, 30.0, -0.84),\n",
    "       (10, 158.0, 416.0, 7.0, 16.0, -0.69),\n",
    "       (11, 165.0, 393.0, 5.0, 14.0, 0.3),\n",
    "       (12, 201.0, 442.0, 5.0, 25.0, -0.46),\n",
    "       (13, 157.0, 317.0, 5.0, 52.0, -0.03),\n",
    "       (14, 131.0, 311.0, 6.0, 16.0, 0.5),\n",
    "       (15, 166.0, 400.0, 6.0, 34.0, 0.73),\n",
    "       (16, 160.0, 337.0, 5.0, 31.0, -0.52),\n",
    "       (17, 186.0, 423.0, 9.0, 42.0, 0.9),\n",
    "       (18, 125.0, 334.0, 8.0, 26.0, 0.4),\n",
    "       (19, 218.0, 533.0, 6.0, 16.0, -0.78),\n",
    "       (20, 146.0, 344.0, 5.0, 22.0, -0.56)], \n",
    "      dtype=[('ID', '<i4'), ('x', '<f8'), ('y', '<f8'), ('sigma_x', '<f8'), ('sigma_y', '<f8'), ('rho_xy', '<f8')])\n",
    "\n",
    "# Save the data to a file\n",
    "np.savez('line_data.npz',line_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. My solution to the challenge from Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See code and comments in `w1challenge.py`. This is written as a python script that can be either imported *or* run directly from the command line, as we'll see in the next few cells.\n",
    "\n",
    "Some of the key points of the exercise were:\n",
    " - random sampling from a normal distribution: use `np.random.normal`\n",
    " - accumulating the components onto a grid: make an array to represent each component, then add them together.\n",
    " - getting the orientation right in imshow: understand `origin=lower` _and_ the array.T operator\n",
    " - shrinking the colourbar using the `shrink=` argument.\n",
    " - introducing viridis, one of the perceptually uniform colourmaps in matplotlib (suitable for those with colourblindness, and for printing in black and white).\n",
    " \n",
    "There's obviously more than one reasonable solution to this problem, but the solutions should be short and should produce something that looks very much like the following image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run w1challenge.py ./challenge_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there is a block at the end of the file defined by an `if` statement: `if __name__ == '__main__'`. \n",
    "\n",
    "> ```python \n",
    "if __name__ == '__main__':\n",
    "     input_file = sys.argv[1] # Get the first command line argument\n",
    "     plot_challenge(input_file)```\n",
    "\n",
    "The code in this special block will be executed if the file is run from the command line (or with `%run` in IPython, which is the same thing), but **not** if the file is imported by another python script or module. If you put code at the end of the file without this block, it would be executed every time the file was imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import w1challenge\n",
    "# Nothing happens!\n",
    "# But we can call the function w1challenge.plot_challenge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `'__main__'` is a string, but `__name__` is the name of a variable. The `__` is **two** underscores. Here is [a post about this](http://stackoverflow.com/questions/419163/what-does-if-name-main-do) on StackOverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Command line arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example introduces another feature: using `sys.argv` to get arguments from the command line (in this case, the name of the input file). `sys.argv` is a list created by Python when it starts to execute a script. \n",
    "\n",
    "    - sys.argv[0] is always the name of the script\n",
    "    - sys.argv[1] is the first command line argument (if there are any)\n",
    "    - sys.argv[2] is the second command line argument, etc.\n",
    "    \n",
    "Anything much more complicated than using `sys.argv[1]` directly (for example, having optional arguments, variable numbers of arguments, arguments that are lists, etc. etc.) is better done with the `argparse` standard library module, which we'll introduce later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Other useful standard library modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These modules and functions aren't essential, but they make life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 `time` and `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(time.time.__doc__)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time.sleep(2) # This function does nothing for 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "time.sleep(5)\n",
    "t2 = time.time()\n",
    "print('I was asleep for {elapsed_time:f} seconds'.format(elapsed_time = t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this is a good way to time how long any bit of Python code takes to run. \n",
    "\n",
    "In IPython there is a magic function called `%timeit`. This runs whatever comes after it a number of times (it works out how many, automatically) and computes the average time it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.random.random(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.random.normal(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 `argparse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('input_file')\n",
    "parser.add_argument('-o','--option', default=42, type=int)\n",
    "parser.add_argument('-t','--logical_option', action='store_true')\n",
    "\n",
    "args = parser.parse_args(['myfile.txt','-o','42'])\n",
    "\n",
    "print(args)\n",
    "print(args.input_file)\n",
    "print(args.option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal usage, the above would usually appear in the `if __name__ == '__main__'` block, and `sys.argv` would be passed as the argument to `parser.parse_args`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 `glob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glob.glob` is a quick and easy way to find all the files matching a simple pattern (this is very common when you're working with simulation outputs or observational data spread across many files and folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all the python notebook files in the current directory\n",
    "glob.glob('./*.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 `subprocess`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've see how to call shell commands from IPython using `!`, but what if we need to talk to the shell from an ordinary Python script? In many cases we don't need to, because the Python standard library has its own versions of some of the common shell commands. \n",
    "\n",
    "The more general solution is the `subprocess` module. [Here's a longer description of this module](https://pymotw.com/2/subprocess/). A common use for this is to call codes like `galfit` or `sextractor` from within a Python script.\n",
    "\n",
    "This is very simple example, that just uses `subprocess` to list the output of the current directory (i.e. to do `ls -l`). Notice you need to split the arguments into a list (investigate the `shell=True` option if making this list is a problem for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "ls_output = subprocess.check_output(['ls',\"-l\"])\n",
    "print(ls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. More matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we made a scatter plot of some points with a Gaussian distribution of points in (x,y). Here's the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.normal(loc=50,scale=20,size=1000)\n",
    "y = np.random.normal(loc=x+5,scale=10,size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the scatter plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_b = pl.figure(figsize=(2.5,2.5))\n",
    "ax = pl.gca()\n",
    "ax.scatter(x,y,s=1,edgecolor='None',c='purple',label='$z=0$')\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_ylim(0,100)\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows a simple way to plot a histogram of the `x` coordinates of these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(2.5,2.5))\n",
    "\n",
    "bin_edges_x = np.arange(0,110,10)\n",
    "hx, junk_bins = np.histogram(x,bins=bin_edges_x) # junk_bins is just equal to bin_edges_x here\n",
    "pl.plot(bin_edges_x[:-1],hx,drawstyle='steps-post',c='k')\n",
    "ax = pl.gca()\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$N(x)$');\n",
    "\n",
    "# Just for this example, plot the edges of the bins as well\n",
    "for bin_edge in bin_edges_x[:-1]:\n",
    "    pl.axvline(bin_edge,c='lightgrey',lw=0.5,zorder=-10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the array of bin *edges* we use for the histogram is one element larger than the histogram itself, because the bin edges include the right-most edge. We have to chop this off with `[:-1]` when we pass the array of edges to `plot`. The option `steps-post` to `plot` tells it that each value in the array of `x` coordinates (i.e. `bin_edges_x`) is the *left-hand* side of a bin that extends to the next value.\n",
    "\n",
    "I prefer that way of doing it because it's simple, but there are alternatives. For example, you could use the `hist` function of `matplotlib`, which is just as easy to use but draws more 'stuff' on the plot (a bunch of rectangles, rather than a line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(2.5,2.5))\n",
    "h,junk_bins,patches = pl.hist(x,bins=np.arange(0,110,10),color='red')\n",
    "\n",
    "ax = pl.gca()\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$N(x)$');\n",
    "\n",
    "# Just for this example, plot the edges of the bins as well\n",
    "for bin_edge in bin_edges_x[:-1]:\n",
    "    pl.axvline(bin_edge,c='lightgrey',lw=0.5,zorder=-10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice that `hist` returns the `patches` (rectangles) used to make the plot, as well as the histogram and the bins that you get from `np.histogram`.) \n",
    "\n",
    "You can make the matplotlib version draw a line instead using the option `histtype='step'`, as in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to normalize histograms such that the total area underneath them is equal to 1. You can do this with the `normed=True` keyword argument (either in `numpy` or `matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(2.5,2.5))\n",
    "\n",
    "h_matplot,junk_bins,patches = pl.hist(x,bins=np.arange(0,110,10),color='red',normed=True,histtype='step')\n",
    "\n",
    "ax = pl.gca()\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$N(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the generating distribution (the Gaussian we sampled `x` from in the first place) over the top of the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(2.5,2.5))\n",
    "\n",
    "h_matplot,junk_bins,patches = pl.hist(x,bins=np.arange(0,110,10),color='red',normed=True,histtype='step')\n",
    "\n",
    "# Import a useful routine to work with the normal distribution from the scipy package.\n",
    "from scipy.stats import norm\n",
    "xx = np.arange(0,100.1,0.1) # we need a lot of x values to make a smooth distribution\n",
    "# norm.pdf(x,mean,sigma) gives the value of the normal distribution at x\n",
    "pl.plot(xx,norm.pdf(xx,50,20.0),c='k',drawstyle='steps-post',ls='solid')\n",
    "pl.axvline(50,c='grey',ls='--') # helpful line at the mean of the distribution\n",
    "\n",
    "ax = pl.gca()\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$N(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing for this example: what if we wanted to plot the underlying Gaussian distribution at the same resolution as our binned `x` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pl.figure(figsize=(2.5,2.5))\n",
    "\n",
    "h_matplot,junk_bins,patches = pl.hist(x,bins=np.arange(0,110,10),color='red',normed=True,histtype='step')\n",
    "\n",
    "from scipy.stats import norm\n",
    "# Smooth version as above\n",
    "xx = np.arange(0,100.1,0.1)\n",
    "pl.plot(xx,norm.pdf(xx,50,20.0),c='k',drawstyle='steps-post',ls='solid')\n",
    "# Blocky version\n",
    "pl.plot(bin_edges_x,norm.pdf(bin_edges_x,50,20.0),c='b',drawstyle='steps-post')\n",
    "\n",
    "pl.axvline(50,c='grey',ls='--') # helpful line at the mean of the distribution\n",
    "\n",
    "ax = pl.gca()\n",
    "for i in [0,-1]:\n",
    "    pl.setp(ax.get_xticklabels()[i],visible=False)\n",
    "    pl.setp(ax.get_yticklabels()[i],visible=False)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$N(x)$');\n",
    "pl.xlim(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue histogram doesn't line up... see if you can fix it. \n",
    "\n",
    "This is quite an easy mistake to make, but the solution isn't as easy as it looks. Clue: you need to add one line to make different values of x to give to `norm.pdf`, and use these in the blocky-version `plot` line together with a change to one of the optional arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above example used two keyword arguments available in most `matplotlib` routines: \n",
    "- `alpha`: changes the transparency of lines or markers (`alpha=0` transparent, `alpha=1` solid)\n",
    "- `zorder`: changes the relative ordering of lines or markers (objects with high `zorder` are drawn on top of those with lower `zorder`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Legends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to make a legend automatically by adding 'label=' arguments in routines that add points and lines to plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "pl.scatter(line_data['x'],line_data['y'],marker='s',edgecolor='None',c='k',label='Positive')\n",
    "pl.scatter(-line_data['x'],-line_data['y'],marker='s',edgecolor='None',c='r',label='Negative')\n",
    "pl.legend(loc='lower right') # Loc is the location of the legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the look of the legend using its many options. Here I get rid of the box around it, change the symbols to be big squares rather than the default of three little squares, and put the labels on the left-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "pl.scatter(line_data['x'],line_data['y'],marker='s',edgecolor='None',c='k',label='Positive')\n",
    "pl.scatter(-line_data['x'],-line_data['y'],marker='s',edgecolor='None',c='r',label='Negatiave')\n",
    "pl.legend(loc='lower right',frameon=False,scatterpoints=1,markerscale=2,\n",
    "          markerfirst=False,handletextpad=0.2,fontsize=10);\n",
    "\n",
    "# Let's fix the x tick labels while we're at it...\n",
    "ax = pl.gca()\n",
    "pl.setp(ax.get_xticklabels(),rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to have even more control over the legend. You can build your own legend by passing explicit `handles` and `labels` as the first two arguments to `legend`. The `labels` are just the text for each entry. The `handles` are 'primitive' matplotlib objects -- for example, a line is `Line2D`. This allows us to control the order of the legend without changing the order of the plot, and (for example) to have a line in the legend even though the points on the plot were made with `scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "pl.scatter(line_data['x'],line_data['y'],marker='s',edgecolor='None',c='k')\n",
    "pl.scatter(-line_data['x'],-line_data['y'],marker='s',edgecolor='None',c='r')\n",
    "\n",
    "ax = pl.gca()\n",
    "pl.setp(ax.get_xticklabels(),rotation=45);\n",
    "\n",
    "# Make a list of handles and a list of labels (in the same order).\n",
    "handles = list()\n",
    "labels  = list()\n",
    "\n",
    "handles.append(pl.Line2D([0],[1],c='r',lw=4))\n",
    "labels.append('Negative')\n",
    "\n",
    "handles.append(pl.Line2D([0],[1],c='k',lw=4))\n",
    "labels.append('Positive')\n",
    "\n",
    "pl.legend(handles,labels,loc='lower right',frameon=False,scatterpoints=1,markerscale=2,\n",
    "          markerfirst=False,handletextpad=0.2,fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This even allows us to put two legends on the same plot, **but** we need to use a trick, otherwise `matplotlib` removes the first legend when we plot the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "pl.scatter(line_data['x'],line_data['y'],marker='s',edgecolor='None',c='k')\n",
    "pl.scatter(-line_data['x'],-line_data['y'],marker='s',edgecolor='None',c='r')\n",
    "\n",
    "ax = pl.gca()\n",
    "pl.setp(ax.get_xticklabels(),rotation=45);\n",
    "\n",
    "# Make a list of handles and a list of labels (in the same order).\n",
    "handles = list()\n",
    "labels  = list()\n",
    "\n",
    "handles.append(pl.Line2D([0],[1],c='r',lw=4))\n",
    "labels.append('Negative')\n",
    "handles.append(pl.Line2D([0],[1],c='k',lw=4))\n",
    "labels.append('Positive')\n",
    "\n",
    "# Notice we store whatever gets returned from legend in a variable here\n",
    "legend1 = pl.legend(handles,labels,loc='lower right',frameon=False,scatterpoints=1,markerscale=2,\n",
    "          markerfirst=False,handletextpad=0.2,fontsize=10);\n",
    "\n",
    "# For the sake of this example, just plot the same legend again in a different place. \n",
    "# This removes legend1 from the plot!\n",
    "legend2 = pl.legend(handles,labels,loc='upper left',frameon=False,scatterpoints=1,markerscale=2,\n",
    "          markerfirst=False,handletextpad=0.2,fontsize=10);\n",
    "\n",
    "# Trick to get the first legend back:\n",
    "ax = pl.gca() # get the axes object\n",
    "ax.add_artist(legend1) # put the first legend back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Grids of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to make a grid of plots using the `gridspec` module. There is a more 'low level' way to do this that use `pl.subplot` directly without importing `gridspec`, but I don't recommend that, because I find gridspec is much easier to use. Be warned, over time `matplotlib` has introduced (too many) different ways to do this, so there is a lot of old, confusing advice on the web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "figure = pl.figure(figsize=(4,4*3.0/2.0))\n",
    "\n",
    "nrow, ncol = 3,2\n",
    "gs         = gridspec.GridSpec(nrow,ncol,wspace=0.0,hspace=0.0)\n",
    "\n",
    "# This GridSpec object can be indexed with [] like a list or array.\n",
    "# The wspace=0 and hspace=0 remove all the space between the panels.\n",
    "# Set them to something larger (e.g. 1.0) if you want space between\n",
    "# the panels.\n",
    "\n",
    "# For later reference, I like to store each panel of the plot in a list\n",
    "axes = list()\n",
    "\n",
    "# Loop over rows and columns\n",
    "for irow in xrange(0,nrow):\n",
    "    for icol in xrange(0,ncol):\n",
    "        # Pass the GridSpec object for this row and column to pl.subplot\n",
    "        # This creates and returns a new axes object representing this panel.\n",
    "        ax = pl.subplot(gs[irow,icol])\n",
    "    \n",
    "        # Plot something on this axis. In this case, we'll just write the\n",
    "        # row and column as a text label (explained below)\n",
    "        pl.text(0.3,0.5,'Axis (%d,%d)'%(irow,icol),transform=ax.transAxes)\n",
    "        axes.append(ax) # store this axes object in our list\n",
    "\n",
    "# Now format all the axes -- for example, get rid of the labels on the\n",
    "# x-axis, except for the bottom row. \n",
    "\n",
    "# I like to put this formatting at the end, but it could be\n",
    "# included along with the loop that makes the plots above. \n",
    "\n",
    "# There are lots of ways to do this. This way should work \n",
    "# regardless of the values of nrow and ncol.\n",
    "axes       = np.array(axes).reshape(nrow,ncol) # reshape the list we made to match the shape of the plot\n",
    "# This will be True for the bottom row, where we \n",
    "has_xtick  = np.repeat(True,nrow*ncol).reshape(nrow,ncol)\n",
    "has_xtick[:-1,:] = False\n",
    "has_ytick  = np.repeat(True,nrow*ncol).reshape(nrow,ncol)\n",
    "has_ytick[:,1:] = False\n",
    "\n",
    "for iax in xrange(0,6):\n",
    "    # This is an alternative to a nested loop over irow, icol\n",
    "    gridcoord = np.unravel_index(iax,(nrow,ncol))\n",
    "    # Hide x tick labels, except for the bottom row\n",
    "    pl.setp(axes[gridcoord].get_xticklabels(),visible=has_xtick[gridcoord])\n",
    "    # Hide y tick labels, except for the left hand column \n",
    "    pl.setp(axes[gridcoord].get_yticklabels(),visible=has_ytick[gridcoord])\n",
    "    # Trim the tick labels from the end of each axis\n",
    "    pl.setp(axes[gridcoord].get_xticklabels()[0],visible=False)\n",
    "    pl.setp(axes[gridcoord].get_xticklabels()[-1],visible=False)\n",
    "    pl.setp(axes[gridcoord].get_yticklabels()[0],visible=False)\n",
    "    pl.setp(axes[gridcoord].get_yticklabels()[-1],visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we wrote text labels using `pl.text(x,y,label)`. We included the optional argument `transform=ax.transAxes`. This 'magic' lets us write `x` and `y` for the text as fractions of the extent of each axis (i.e. numbers between 0 and 1). If we didn't, we would have to give `x` and `y` in the units of the axis, which is usually not what we want to do, unless we're putting labels near specific data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fill Between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example should be obvious when you've made the plot. If you use this in more complicated situations and it comes out looking ugly, have a look at the `where` argument to `fill_between`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x  = np.arange(-2*np.pi,2*np.pi,0.01)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.sin(x+np.pi/2.0)\n",
    "\n",
    "pl.figure(figsize=(8,2))\n",
    "pl.plot(x,y1,c='k')\n",
    "pl.plot(x,y2,c='r')\n",
    "pl.fill_between(x,y1,y2,color='c',alpha=0.2)\n",
    "\n",
    "# Notice what's going on with the '2*list' here\n",
    "pl.fill_betweenx([-1,1],2*[2*np.pi/2.0],2*[2.5*np.pi/2.0],color='m',alpha=0.2)\n",
    "\n",
    "pl.xlim(-2*np.pi,2*np.pi)\n",
    "pl.ylim(-1,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. More `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complete discussion of the basic use of `numpy` is given [here](http://www.scipy-lectures.org/intro/numpy/operations.html). These are all things that, based on my experience, you will end up using all the time. Obviously this can only cover the most basic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Copies vs. references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered this for basic Python data types last week, and saw the `copy` module. Numpy arrays have their own in-built `copy` function which does the same thing, and is the best way to copy them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros(3,dtype=np.float32)\n",
    "y = x\n",
    "z = x.copy()\n",
    "x[2] = 100.0 # set an element of x\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "print('z = {}'.format(z))\n",
    "print('y is x? {}'.format(y is x))\n",
    "print('z is x? {}'.format(z is x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that `y=x` makes `y` a reference to `x`, rather than a copy, so updating `y` also updates `x`. In practice it can get quite hard to guess whether any of the hundreds of functions in `numpy` will return a copy or a reference to the array you give it as input (in some cases this even depends on the input!).\n",
    "\n",
    "Bottom line: when you really want a copy, explicitly make a copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Sorting arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two simple ways to sort an array `x`: either `np.sort(x)`, which returns a copy of `x` in sorted order, or `x.sort()`, which sorts x 'in place' -- it actually moves the values in `x` itself into sorted order.\n",
    "\n",
    "This choice between `np.something(x)` (copy) and `x.something()` (in place change to `x`) is similar for a lot of `numpy` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randint(0,100,size=10) # Some random integers\n",
    "print('Original x: {}'.format(x))\n",
    "print('np.sort(x): {}'.format(np.sort(x))) # np.sort returns a copy\n",
    "print('         x: {}'.format(x))\n",
    "print('  x.sort(): {}'.format(x.sort()))   # x.sort() returns None...\n",
    "print('         x: {}'.format(x))          # but now x is sorted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.randint(0,100,size=10) # Some random integers\n",
    "a = np.argsort(x)\n",
    "print(a)          # The *indices* rather than the *values* in sorted order\n",
    "print(x[a])       # Use the indices to sort the array\n",
    "print(x[a[::-1]]) # Sort backwards by reversing the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful because you can now use the indices `a` to put any other array in the same order, not just `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Concatenating, stacking and splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all ways to combine multiple arrays into a single array. `vstack` and `hstack` add rows and columns respectively, increasing the dimension of the array (i.e. making two 1d arrays into one 2d array). `vstack` means 'vertical stack' (add more rows) and `hstack` means horizontal stack (add more columns) -- experiments are the easiest way to understand what the difference is. \n",
    "\n",
    "`concatenate` joins arrays end-to-end along a particular dimension (by default, the first one). For example, it's the standard way to combine a list of 1d arrays into one big 1d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(10,25)\n",
    "y = np.arange(30,45)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}'.format(y))\n",
    "print('concatenated: {}'.format(np.concatenate((x,y))))\n",
    "\n",
    "v = np.vstack((x,y))\n",
    "print('shape after vstack: {}'.format(u.shape))\n",
    "\n",
    "h = np.hstack((x,y))\n",
    "print('shape after hstack: {}'.format(v.shape))\n",
    "\n",
    "# In this case, stacking horizontally is the same as concatenating \n",
    "print(np.all(h == np.concatenate([x,y])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also very similar functions to split up arrays into smaller arrays. Following on from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b = np.vsplit(v,2)\n",
    "np.all(a==x) and np.all(b==y) # We should be back to where we started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Creating arrays, repeat, reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [loads of different functions](https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html) for creating arrays in `numpy`. These include `zeros` to make an empty array and `repeat` to make an array where every element has a particular value. We saw these last week.\n",
    "\n",
    "Let's examine how long it takes to create a big array of -1's, using IPython's `%timeit` magic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.zeros(1000) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.repeat(-1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, the first way is about 3x faster. This is helpful to know if you're making very big arrays. If you can think of two ways to do something, then either is fine, unless one of them **really** slows down your program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to make an array with a repeating pattern, like `[1,0,-1,1,0,-1,1,0,0,-1]`. Let's try to do that with repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.repeat((1,0,-1),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly that's not the right way to do it. The correct function in this case is `tile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.tile((1,0,-1),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reshape` function can be used if we want a 2d version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.reshape(np.tile((1,0,-1),10), (10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Record arrays (recarrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define your own numpy data types that are combinations of the basic types, with each element of the new composite type associated with a name. This is useful for storing **structured data**, and the most general way to do that with `numpy` (although in many cases you might find the Table class from `astropy` easier to work with).\n",
    "\n",
    "The `line_data` we defined in the first cell is a structured array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_data[0:3] # Show the first three elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can see this has a complicated dtype, made up of 6 elements\n",
    "line_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# line_data is **not** a 20x6 array, it's a 20x1 array of elements of this dtype.\n",
    "line_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want to know the total number of bytes in each element, you can:\n",
    "line_data.dtype.itemsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The important thing* is that individual fields can be accessed by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_data['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a record array like this from scratch, use the function `numpy.dtype`, passing a ***list of tuples*** of the form (`name_of_field`, `data_type_of_field`). The data type can either be an existing `dtype`, like `numpy.int32`, or a string. `numpy` will convert the string into a dtype, if it can. The '<' and '>' indicated 'little endian' or 'big endian' representations. For example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dtype = np.dtype([('ID', '|S16'), ('ra', '<f8'), ('dec', '<f8')])\n",
    "np.zeros(4,dtype=my_dtype) # Make an empty array with four of these elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `|S16` is the way to specify a string of length 16 characters as a datatype. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Logical masks and crazy floating point values (`nan` and `inf`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make an array of booleans that indicate whether each element in the array meets some criterion we define. These are often called 'masks'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = (line_data['x'] > 200.0) & (line_data['y'] > 500.0)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can 'filter' the array by using the mask as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line_data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw last week, we can use `numpy.where` to find the actual indices of the elements for which the mask is `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.where(mask)[0]\n",
    "print(w)\n",
    "line_data[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often you'll end up doing something like taking the `log10` of an array with some negative numbers or dividing an array by another array with zeros in it. In those cases, the special values `nan` (not a number), `inf` (infinity) and `-inf` (negative infinity) will show up in the results, mixed in with the elements where whatever operation you tried *was* mathematically valid. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crazy_values = np.log10(np.tile((1,0,-1),10)) # This will cause some warnings to appear!\n",
    "print(crazy_values[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` has some routines to make masks of the elements that contain these special values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(np.isinf(crazy_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(np.isnan(crazy_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `isfinite` routine returns True for the elements in its argument that are *not* any of these special values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(np.isfinite(crazy_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Further reading:\n",
    "\n",
    "For advanced/frequent work with masked arrays, there is also the `numpy.ma` module, which defines a new 'masked array' type. This gets quite complicated to work with and I don't use it very often. It will be more useful if you are doing complicated linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 `numpy.where` with 2d arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we saw that (for example) `np.where(x>something)` returns a tuple of length 1 if `x` is a 1d array, so to get the actual elements of x that are `>something` we need to get the first (and only) element of the result: `np.where(x>something)[0]`.\n",
    "\n",
    "What if `x` is 2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(9).reshape((3,3))\n",
    "print(x.shape)\n",
    "w = np.where(x > 4)\n",
    "print(w) # note this is a tuple of length 2\n",
    "print(x[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the unique values in an array, use `numpy.unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0,10,20) # 20 random integers between 0 and 10\n",
    "print(x)\n",
    "print(np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results are sorted. The `numpy.unique` function has some useful optional arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u, idx, inv, counts = np.unique(x,return_index=True,return_inverse=True,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check what these do yourself. One useful application is to make a list of lengths and offsets of values in a sorted array. Here's a trivial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0,10,200) # 20 random integers between 0 and 10\n",
    "a = np.argsort(x)\n",
    "values, offset, length = np.unique(x[a],return_index=True,return_counts=True)\n",
    "for u,o,l in zip(values,offset,length):\n",
    "    print('{} | {}'.format(u, x[a][o:o+l] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that `x` is a label that groups some other properties: for example, the 'galaxy ID' of particles in a cosmological simulation or the 'source ID' of multiple exposures of a set of targets observed with a spectrograph. The lengths and offset arrays can be used to find the properties of the subsets defined by `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "luminosity = np.random.random(200) # Some property associated with each x from the last cell\n",
    "\n",
    "print('  | Total Luminosity')\n",
    "for u,o,l in zip(values,offset,length):\n",
    "    print('{} | {}'.format(u, np.sum(luminosity[a][o:o+l])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are even more general ways to do something similar (the `pandas` package has many such functions) and more efficient techniques with `numpy`, to get rid of that `for` loop (too advanced for this tutorial, but if you want more details have a look at the documentation for `numpy.ufunc.reduceat` and google for some examples, e.g. of `add.reduceat`). The above example works well in most simple cases. Remember the array has to be sorted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Testing if values are in another array using in1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy.in1d` provides a quick way to check if each element of an array is in another array, but it doesn't tell you *where* those elements are in the other array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "odd_numbers = np.arange(1,100,2)\n",
    "np.in1d([1,2,3,4,5],odd_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiently finding the index of each matching element in the other array is the point of the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Finding locations of specific values in arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching specific values in very long lists of arbitrary integers is a common task; for example, in working with the particle data from cosmological simulations, you might might have a list of particle IDs in one snapshot of the simulation, and want to find the same particles in another snapshot.\n",
    "\n",
    "Currently, this operation isn't covered by an existing routine in `numpy` or `scipy` (as far as I know!). The nearest thing is in the `pandas` statistics package, which we'll look at next week. However, it's not *too* hard to write such a routine yourself. This is the basis of one of the challenges.\n",
    "\n",
    "First, we'll make an array of 1000 unique integer values, `BIG_ARRAY`. Since we want the values to be unique but not sorted for this example, this takes a couple of steps to set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x         = np.random.randint(0,500,200) # 200 random integers between 0 and 500\n",
    "BIG_ARRAY = np.unique(x)\n",
    "\n",
    "# Note this trick for getting a random ordering by sorting random numbers\n",
    "random_order = np.argsort(np.random.random(len(BIG_ARRAY)))\n",
    "BIG_ARRAY    = BIG_ARRAY[random_order]\n",
    "\n",
    "BIG_ARRAY    = BIG_ARRAY + 2000 \n",
    "print(BIG_ARRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generated 200 random integers between 0 and 500, find only the unique values, then randomize their order. To make it clear that the numbers can be anything for this example, I added 2000 to each of them. \n",
    "\n",
    "Now let's say we wanted to find where the value 2059 was in this array. Obviously we could use `where`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(BIG_ARRAY == 2201)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we wanted to find more values, we might use a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset the seed, so we get consistent results each time we try this\n",
    "np.random.seed(232)\n",
    "small_array = np.random.randint(2100,2300,10) # some values to look for: not all of them will be found!\n",
    "\n",
    "# Loop\n",
    "matches = list()                                    \n",
    "for s in small_array:\n",
    "    w = np.where(BIG_ARRAY == s)[0]\n",
    "    if len(w) == 0:\n",
    "        # If we didn't find anything, store -1\n",
    "        matches.append(-1)\n",
    "    else:\n",
    "        matches.append(w[0])\n",
    "    \n",
    "matches = np.array(matches)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if either `small_array` or `BIG_ARRAY` are large, this is a very slow way to do it, because `np.where` has to compare each value from `small_array` against *every* value in BIG_ARRAY. In most of these cases, we didn't even find a match.\n",
    "\n",
    "**Solution:** When we're dealing with large arrays, it's usually more efficient to sort `BIG_ARRAY` and use a [**bisection search**](https://en.wikipedia.org/wiki/Binary_search_algorithm). In `numpy` we can do this with the `searchsorted` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First sort BIG_ARRAY\n",
    "sorting_idx = np.argsort(BIG_ARRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now use searchsorted (have a look at the docstring for this routine as well)\n",
    "\n",
    "# What is the *smallest* index in the sorted BIG_ARRAY where I can insert the value '2101'\n",
    "# without changing the order of BIG_ARRAY?\n",
    "print(BIG_ARRAY[sorting_idx].searchsorted(2201,side='left'))\n",
    "\n",
    "# What is the *largest* index in the sorted BIG_ARRAY where I can insert the value '2101'\n",
    "# without changing the order of BIG_ARRAY?\n",
    "print(BIG_ARRAY[sorting_idx].searchsorted(2201,side='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just to demonstrate that's the case, show the indices around 56-57\n",
    "BIG_ARRAY[sorting_idx][[59,60,61,62]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that doesn't directly tell us which of the indices (56 or 57) is *equal* to  2189, or even that **either** of them are. However, look what happens when we try to find a value that isn't in `BIG_ARRAY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(BIG_ARRAY[sorting_idx].searchsorted(2202,side='left'))\n",
    "print(BIG_ARRAY[sorting_idx].searchsorted(2201,side='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be enough information to get started with the 'matching values in huge arrays of integers' challenge at the end of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy` package has more complicated numerical procedures that work (mostly) with `numpy` arrays. I've only ever used a small fraction of them! The following are examples of some `scipy` routines I find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Finding neighbours with KD Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KDTree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html) (a class defined in `scipy.spatial.kdtree`) provides an efficient way to search for neighbours of points in large sets of coordinates. This example shows how to do this for a 2D array of coordinates (it also shows how to draw a circle on a plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.kdtree import KDTree\n",
    "\n",
    "np.random.seed(42)\n",
    "xy = np.random.normal(1,1,(100,2))\n",
    "kd = KDTree(xy) # construct the tree by passing an array of coordinates\n",
    "\n",
    "# Show the points on a scater plot\n",
    "pl.figure(figsize=(3,3))\n",
    "pl.scatter(xy[:,0],xy[:,1],s=2,edgecolor='None',c='k')\n",
    "\n",
    "# Find all the points within radius r=1 of the first point\n",
    "radius = 1\n",
    "idx  = kd.query_ball_point(xy[0,:],radius) # returns the index of each point in xy within r<=1 of element 0\n",
    "\n",
    "# Highlight the neighbours on the plot\n",
    "pl.scatter(xy[0,0],xy[0,1],c='r',s=5,zorder=10,edgecolor='None')\n",
    "for i in idx[:]:\n",
    "    if i != 0:\n",
    "        pl.scatter(xy[i,0],xy[i,1],c='lime',s=20,edgecolor='None',zorder=0)\n",
    "\n",
    "# Draw a circle of radius 1\n",
    "ax = pl.gca()\n",
    "ax.add_artist(pl.Circle([xy[0,0],xy[0,1]],radius=radius,color='purple',alpha=0.1,zorder=-1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other functions and techniques associated with the KDTree object.\n",
    "\n",
    "In the same package there is also `cKDTree`, which is an optimized version of `KDTree` for very large datasets, with fewer functions than `KDTree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you're familiar with the basic idea of interpolation. The `scipy.interpolate.interp1d` function returns another function that takes a set of points `x` and associated 'measured values' `y` and returns an interpolated value of `y` at any intermediate value of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.interpolate as spi\n",
    "\n",
    "# 10 sparsely-sampled 'data' points\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0,2*np.pi,10)\n",
    "pl.scatter(x,np.sin(x),c='k',label='data')\n",
    "\n",
    "# Linearly interpolate between these points.\n",
    "linear_interp = spi.interp1d(x,np.sin(x),kind='linear')\n",
    "cubic_interp  = spi.interp1d(x,np.sin(x),kind='cubic')\n",
    "\n",
    "xx  = np.linspace(x.min(),x.max(),100)\n",
    "_xx = np.linspace(-np.pi/2.0,2.5*np.pi) # Show true curve over a wider range in x\n",
    "pl.plot(_xx,np.sin(_xx),c='k',ls='--',label='(true)')\n",
    "pl.plot(xx,linear_interp(xx),c='r',label='linear interpolation')\n",
    "pl.plot(xx,cubic_interp(xx),c='b',label='cubic interpolation')\n",
    "\n",
    "pl.legend(loc='upper right',frameon=False);\n",
    "pl.xlim(0,2*np.pi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the documentation for `interp1d`. What happens if you try to use the interpolating function for values smaller than `x.min()` or larger than `x.max()`? How can you change that behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common use of interpolation is to invert functions numerically -- i.e. to construct a continuous function that returns `x(y)` for any `y` using data that sample `y(x)` at a few values of `x`. (Obviously the functions have to be single-valued for that to work, which isn't the case for `sin(x)`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple example numerically integrates `y=x**2` between 4 and 6, just to demonstrate how to do this with `scipy`. Practical uses of numerical integration often turn out to be a lot more complicated!\n",
    "\n",
    "The main point of this example is that numerical integration by Gaussian quadrature usually works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.integrate as spint\n",
    "\n",
    "x = np.linspace(0,10,100)\n",
    "\n",
    "pl.figure(figsize=(3,3))\n",
    "pl.plot(x,x**2);\n",
    "pl.fill_between(x,x**2,0,where=((x>=4) & (x<=6)),alpha=0.4,color='c')\n",
    "\n",
    "# The analytic solution\n",
    "analytic = lambda x: (1/3.0)*x**3\n",
    "print('Analytic solution: {}'.format(analytic(6)-analytic(4)))\n",
    "\n",
    "# The numerical solution using adaptive gaussian quadrature\n",
    "func = lambda t: t**2 # the function to integrate\n",
    "integral_quad, quad_err = spint.quad(func,4,6)\n",
    "print('Quad. solution:    {}'.format(integral_quad))\n",
    "\n",
    "# The numerical solution using samples of the function\n",
    "mask = (x>=4) & (x<=6)\n",
    "print('Number of samples in interval = %d'%(np.sum(mask)))\n",
    "integral_simps = spint.simps(x[mask]**2,x[mask])\n",
    "print('Simpson solution:  {}'.format(integral_simps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware of that functions exist to do Romberg integration: `scipy.integrate.romberg` (continuous functions) and `scipy.integrate.romb` (same thing for discrete samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Astropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`astropy`](http://www.astropy.org/) is a large third-party Python package written as a collaborative effort among many astronomers. It provides a lot of functions useful in many astronomical tasks, including things like handling spherical coordinates in different systems and reading arrays of data in common formats (including astro-specific things like FITS headers that define exotic coordinate systems, and VOTables). \n",
    "\n",
    "`astropy` is in continuous and active development, although the core parts are now quite stable. Don't trust `astropy` to be absolutely bullet-proof for anything you really care about, but do use it to avoid re-inventing the wheel. \n",
    "\n",
    "Also, if you use it for work that goes into a paper, you should mention it in the Acknowledgements. Strictly speaking this should also go for `numpy`, `scipy`, `matplotlib` and anything else you use that was written by someone else. This is still not as widespread as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Astropy's `Table` class is a very useful for dealing with most astronomical data in tabular form, particularly that read from FITS files, a common 'structured binary' format developed for astronomy and widely used for most observational data. Full documentation [here](http://docs.astropy.org/en/stable/table/), including a 'Getting Started' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tables; reading and writing FITS\n",
    "from astropy.table import Table, Column\n",
    "\n",
    "# It's easy to turn a numpy recarray into a Table\n",
    "t = Table(line_data)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to save tables as FITS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.write('line_data.fits',overwrite=True)\n",
    "t = Table.read('line_data.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated tasks with FITS, there is the `astropy.io.fits` module, which is well explained in [this tutorial](http://www.astropy.org/astropy-tutorials/FITS-images.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Sky coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the functions for spherical coordinate conversions in `astropy`, we're going to use an N-body model of the [Sagittarius stellar stream](https://en.wikipedia.org/wiki/Sagittarius_Stream) created by Law & Majewski (2010)  [(data borrowed from the Gaia Challenge workshop, courtesty of A. Price-Whelan)](http://astrowiki.ph.surrey.ac.uk/dokuwiki/doku.php?id=tests:streams:challenges#the_sagittarius_stream). I've already included this in the notebook directory as the file `all_particles.txt`. The next cell reads this data into an astropy Table, via `numpy.genfromtxt`, and prints some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = Table(np.genfromtxt('./all_particles.txt',names=True,delimiter=','))\n",
    "print(len(d))\n",
    "print(d.colnames)\n",
    "print(d[0:5])\n",
    "\n",
    "# Galactic longitude and latitude\n",
    "l,b = d['l'],d['b']\n",
    "\n",
    "print('')\n",
    "print('Ranges of galactic coordinates:')\n",
    "print('{} < l < {}'.format(l.min(),l.max()))\n",
    "print('{} < b < {}'.format(b.min(),b.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you're interested, the data in this table is:\n",
    "    - Cartesian xyz position of each particle in kiloparsecs, with the origin at the Galactic centre\n",
    "    - Cartesian velocity of each particle (3 components, in km/s)\n",
    "    - Galactic longitude l (degrees)\n",
    "    - Galactic latitude b (degrees)\n",
    "    - Distance of the particle (from the Sun?)\n",
    "    - The (Heliocentric?) radial velocity (in km/s)\n",
    "    - The apparent angular proper motion of the particle in l and b (micro-arcseconds per year)\n",
    "    \n",
    "We'll only be using the xyz and lb coordinates. Let's plot these in an un-exciting way so we know roughly what the data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure()\n",
    "pl.scatter(l,b,c='k',s=1,edgecolor='None',label='Sgr stream stars')\n",
    "pl.scatter([180],[0],c='b',label='Galactic anti-centre',zorder=10)\n",
    "pl.legend(loc='upper left',scatterpoints=1,markerscale=2)\n",
    "pl.xlabel('$l$ (deg)')\n",
    "pl.ylabel('$b$ (deg)')\n",
    "pl.xlim(0,360)\n",
    "pl.ylim(-90,90)\n",
    "\n",
    "pl.figure(figsize=(4,4))\n",
    "pl.scatter(d['x'],d['y'],c='k',alpha=0.2,s=1,edgecolor='None',label=None)\n",
    "pl.scatter([0],[0],c='r',label='Galactic centre')\n",
    "pl.xlabel('X (kpc)')\n",
    "pl.ylabel('Y (kpc)')\n",
    "pl.legend(loc='upper left',scatterpoints=1,markerscale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example shows how to convert between the galactic coordinates we have, and the equatorial (RA and DEC) coordinates that would be used to point a telescope. You can read more about `astropy` coordinate conversions in the tutorial [here](http://docs.astropy.org/en/stable/coordinates/) and the functions of the `SkyCoord` object [here](http://docs.astropy.org/en/stable/coordinates/skycoord.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astropy.coordinates as c\n",
    "import astropy.units as u\n",
    "\n",
    "# Make a 'galactic coordinates' object using the data we have. \n",
    "# Note that we need to tell astropy what the units of our coordinates\n",
    "# are using the astropy.units package. We have coordinates in degrees,\n",
    "# so we multiply our numbers for l and b by astropy.units.deg:\n",
    "gal_coords = c.SkyCoord(l*u.deg,b*u.deg,'galactic')\n",
    "\n",
    "# Get the equatorial representation of our galactic coodinates. This is \n",
    "# a 'property' of our galactic coordinates object.\n",
    "# (ICRS is an international standandard equatorial coordinate system)\n",
    "equ_coords = gal_coords.icrs\n",
    "\n",
    "# Do the same thing for the galactic coordinates of the Galactic centre (l,b) = (0,0)\n",
    "# and anti-centre (l,b) = (180,0)\n",
    "gal_centre_equ = c.SkyCoord(0*u.deg,0*u.deg,'galactic').icrs\n",
    "gal_anti_equ   = c.SkyCoord(180*u.deg,0*u.deg,'galactic').icrs\n",
    "\n",
    "# Plot the points in RA and DEC. Note we get these as methods of the coordinates object.\n",
    "pl.figure()\n",
    "pl.scatter(equ_coords.ra,equ_coords.dec,c='k',s=1,edgecolor='None',label='Sgr stream stars')\n",
    "\n",
    "# Plot the galactic centre and anti-centre\n",
    "pl.scatter(gal_centre_equ.ra,gal_centre_equ.dec,s=32,c='r',zorder=10,label='Centre')\n",
    "pl.scatter(gal_anti_equ.ra,gal_anti_equ.dec,s=32,c='c',zorder=10,label='Anti-centre')\n",
    "\n",
    "pl.xlabel('RA (deg)')\n",
    "pl.ylabel('DEC (deg)')\n",
    "\n",
    "pl.legend(loc='lower left',scatterpoints=1,markerscale=2)\n",
    "pl.xlim(0,360)\n",
    "pl.ylim(-90,90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SkyCoord` object has loads of functions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What constellation is the centre of the the Galaxy in?\n",
    "gal_centre_equ.get_constellation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And the anticentre?\n",
    "gal_anti_equ.get_constellation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the equatorial data in an Aitoff projection. Note we have to ensure that the longitude is in radians from -pi to pi, and the latitude is in radians from 0 to +pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8,4.2))\n",
    "pl.subplot(111, projection=\"aitoff\")\n",
    "pl.grid(True)\n",
    "pl.scatter(equ_coords.ra.wrap_at(180 * u.deg).radian, \n",
    "           equ_coords.dec.radian,\n",
    "           s=1,edgecolor='None',alpha=0.3,c='k');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-challenge: draw a line representing the galactic plane on the Aitoff plot above.** This isn't totally straightforward -- you might need to consult the astropy links given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get into good habits when writing the solutions to these challenges: try to write basic docstrings for your functions, and sufficient comments to explain how your code works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Write a 'particle data' package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own package (which only need have one python file in it) that can be used to compute useful things for arrays of Cartesian coordinates and masses that describe 'particles' (for example, particles in an N-body simulation, or stars in a real globular cluster). For example, just to give an example of what I'm thinking about, the following arrays define the positions and masses of 1000 'particles':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "coordinates = np.random.normal(1,1,(1000,3))\n",
    "masses      = np.random.exponential(1,1000)\n",
    "\n",
    "pl.figure(figsize=(2.5,2))\n",
    "pl.scatter(coordinates[:,0],coordinates[:,1],c=masses,s=10,edgecolor='None',alpha=0.8)\n",
    "pl.colorbar(shrink=0.9,label='Mass');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice how the mass array was passed as the 'color' option to the scatter plot there!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your package should be importable, and offer at least the four functions listed below (feel free to vary the arguments and return values, or add extra functions, as long as they get the job done).\n",
    "\n",
    "The following sketch assumes the package is called `pyparticle`, but choose your own name!\n",
    "\n",
    "* **`pyparticle.radii(coordinates,centre)`** : takes an array of 2D or 3D coordinates (it should work for both) and returns the radii of those points from the coordinate `centre`. \n",
    "\n",
    "* **`pyparticle.density_profile(radii,bins,mass=mass)`** : takes an array of radii (from some implicit centre point) and (optionally) an array of masses, and computes the density (either number density or mass density) in radial shells (the edges of which are given as `bins`). The values returned by the function, e.g. `bin_edges` and `density`, should be such that you can do `plot(bin_edges[:-1],density` to show the profile, as you would for the values returned by `np.histogram`.\n",
    "\n",
    "* **`pyparticle.percentile_radius(radii,percentile,mass=mass)`** : takes the radii and (optionally) masses and returns the radius that encloses a given percentage of the particles. For example, the radius that encloses 50% of the particles, or 50% of the mass.\n",
    "\n",
    "* **`pyparticle.r_nth_nearest(coordinates,n)`** : takes an array of coordinates and an integer n, and returns the distance to the n'th nearest neighbour of each particle. This distance is one way of estimating the local density around each particle.\n",
    "\n",
    "You should write some examples to demonstrate how your package works (assume at least 10000 particles), and that it works as expected. \n",
    "\n",
    "Concentrate on making these functions work. If you want you *can* include classes and sophisticated error handling with exceptions, but you certainly don't need them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you've learned in the Statistics course regarding the best way to fit a straight line to data with errors to fit, fit a straight line to this data. Notice that this plot uses the `errorbar` function, which is pretty easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the example data from Hogg et al.\n",
    "data = np.load('line_data.npz')['arr_0']\n",
    "\n",
    "pl.figure(figsize=(4,4))\n",
    "pl.errorbar(data['x'],data['y'],xerr=data['sigma_x'],yerr=data['sigma_y'],\n",
    "            marker='o',linestyle='None',c='k',markeredgecolor='None')\n",
    "pl.xlabel('$x$')\n",
    "pl.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Fancy 2D Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you learned in this week's `matplotlib` section, write a routine `fancy_dotplot(xyz,projection=0)` that takes a 3D array of coordinates and makes something like the scatter plot below by projecting the coordinates along one of the axes (either axis 0, 1 or 2, given as the optional argument `projection`; in the example `projection=0`). \n",
    "\n",
    "<img style=\"float: left;\" src=\"w2challenge_dotplot_0.png\">\n",
    "\n",
    "The histograms on the side of the plot show the density projected along each coordinate axis, while in the background to the scatter plot, the colour scale image shows the density of points along the projected axis (with the same xy bin spacing as the histograms on the sides).\n",
    "\n",
    "As an extension, add a simple way to vary the properties of the dots, the histograms and the colour image by passing optional dictionary arguments for each, keeping the same defaults if nothing is passed (you might need to have a look at the note on 'implicit' argument lists and dictionaries in the 'appendix' to this session below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Matching values in huge arrays of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the `numpy` section above, I introduced the `searchsorted` function to do bisection searches on sorted arrays. The point of this challenge is to put that to use.\n",
    "\n",
    "Write a function `match` that takes two integer arrays, `A` and `B`, and returns an array `M` of the same length as `A` that contains, for each element in `A`, the index of the element with the same value in `B`. If no match is found in `B`, the corresponding element of `M` should be `-1`. You can assume that values in `B` are unique.\n",
    "\n",
    "Demonstrate that your function works to match values (and detect non-matches) in an array `B` of size 100000 (you'll need to generate a unique array of 100000 integers to do that test).\n",
    "\n",
    "Use the `time` (or `%timeit`) functions to compare the performance of your `match` to a loop using `np.where` to match each item, for different combinations of `A` and `B` sizes.\n",
    "\n",
    "This is not an easy challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- IO formats for large numerical datasets: HDF5 and Fortran binary files.\n",
    "- A few more functions from astropy\n",
    "- Healpix\n",
    "- AstroML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are few less important topics that were either partly covered last week or didn't fit in anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Making your own Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered some of this last week.\n",
    "\n",
    "As well as developing packages for specific projects, I find it useful to have a package to collect together various miscellaneous bits of simple code that I use in many different places. I have a package (which I've called `apcpy`) that has code to do things with plots, code to sort lists, code to read particles from Gadget simulation files, code to compute things for NFW profiles etc. etc. \n",
    "\n",
    "This package lives in `\\home\\andrew\\code\\python\\apcpy`, so it can be imported from anywhere as long as `\\home\\andrew\\code\\python` ends up in `sys.path` (which I ensure by including that directory in my `PYTHONPATH`, using `.bashrc`).\n",
    "\n",
    "Remember that to make a package, you just need a directory containing some python files and an `__init__.py` file, which can be empty."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/-- mypackage\n",
    "    |-- __init__.py\n",
    "    |-- alpha.py\n",
    "    |-- beta.py    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Implicit lists of arguments and sets of keyword arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll often see this in Python code, but we didn't cover it last week. As well as explicit definitions of function arguments, you can also ask Python functions to 'grab' any other arguments that get passed to the function using `*args` and `**kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(a,b,c,an_option=0,*args,**kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('Regular arguments {} {} {}'.format(a,b,c))\n",
    "    \n",
    "    print('Regular argument with default: an_option = {}'.format(an_option))\n",
    "    \n",
    "    if len(args) > 0:\n",
    "        print('args is a tuple that holds the remaining non-keyword arguments: {}'.format(args))\n",
    "    \n",
    "    for k,v in kwargs.iteritems():\n",
    "        print('kwargs is a dict that holds keyword arguments: {} = {}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*args` captures any non-keyword arguments after the mandatory ones (in this case a,b,c) and those with defaults (`an_option` here). They are stored in a tuple called `args`.\n",
    "\n",
    "`**kwargs` captures any **keyword arguments** after those you've defined, and stores them in a dictionary called `kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func(1,2,3,5,6,7,8,alpha=1,beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful if you want to 'pass on' arguments from one function to another that it calls. If you refer to `args` with `*args` inside the function, the arguments in `args` will be treated as if you'd typed them out one by one with a comma between them (if you use `args` without the `*`, as in the cells above, then it will be treated as a tuple). A similar logic applies to `**kwargs`.\n",
    "\n",
    "Here's a simple example that provides a more complicated version of the standard `matplotlib` `plot` routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_on_a_figure_of_some_size(xsize,ysize,*args,**kwargs):\n",
    "    pl.figure(figsize=(xsize,ysize))\n",
    "    pl.plot(*args,**kwargs)\n",
    "    pl.show()\n",
    "\n",
    "plot_on_a_figure_of_some_size(2,2,[1,2,3],c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass all the arguments (after the first two) and all the keywords through to the plot routine, so we don't have to worry about copying the many arguments of `plot` one by one.\n",
    "\n",
    "This has lots of other uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is the name for the set of rules that govern what happens when you ask functions in `numpy` to carry out operations that combine arrays with different shapes. Here are three attempts to explain that (all these explanations are quite long, no need to read them right now):\n",
    "\n",
    "http://www.scipy-lectures.org/intro/numpy/operations.html#broadcasting\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc\n",
    "\n",
    "As those links show, the rules for broadcasting and the 'tricks' you can do by understanding them are a deep and complicated topic. You'll need to have some appreciation of broadcasting, if you want do non-trivial maths with multidimensional arrays. If not, then most problems with incompatible shapes of arrays can be fixed by checking the `shape` and dimensionality of your array and use of `np.atleast_1d`, `np.atleast_2d` and `np.transpose` (or `array.T`)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
